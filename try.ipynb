{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4878ba72-dffd-47bc-8a7f-96b800efbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, default_data_collator, get_scheduler, AutoModelForSeq2SeqLM\n",
    "from myReader import collote_fn,DuReaderQG\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_checkpoint = 'uer/t5-base-chinese-cluecorpussmall'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "852ce8b7-0cd6-4ef4-9935-6080290b9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myReader import get_dataLoader,DuReaderQG\n",
    "\n",
    "convert_func = partial(\n",
    "            collote_fn, \n",
    "            tokenizer=tokenizer, \n",
    "            model=model,\n",
    "            max_input_length=512,\n",
    "            max_target_length=64,\n",
    "        )\n",
    "train_data = DuReaderQG('data/DuReaderQG/train.json')\n",
    "valid_data = DuReaderQG('data/DuReaderQG/dev.json')\n",
    "\n",
    "train_dataloader = get_dataLoader(train_data, model, tokenizer, 512, 64, batch_size=4, shuffle=True)\n",
    "valid_dataloader = get_dataLoader(valid_data, model, tokenizer, 512, 64, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "309fb3af-9ca2-471b-8e1b-a7d5ab4cc468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root/autodl-tmp\n",
      "Changed working directory to: /root/autodl-tmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 打印当前工作目录\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# 更改当前工作目录（如果必要）\n",
    "new_dir = '/root/autodl-tmp/'\n",
    "os.chdir(new_dir)\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "# 确保当前目录在系统路径中\n",
    "if new_dir not in sys.path:\n",
    "    sys.path.append(new_dir)\n",
    "\n",
    "# 清除之前的导入缓存\n",
    "if 'myReader' in sys.modules:\n",
    "    del sys.modules['myReader']\n",
    "\n",
    "# 尝试导入 convert_example 函数\n",
    "from myReader import get_dataLoader\n",
    "\n",
    "# # 测试函数\n",
    "# convert_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a31d45-720e-4b56-8c97-c333969bab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python3\n",
    "\"\"\"\n",
    "==== No Bugs in code, just some Random Unexpected FEATURES ====\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┐│\n",
    "││Esc│!1 │@2 │#3 │$4 │%5 │^6 │&7 │*8 │(9 │)0 │_- │+= │|\\ │`~ ││\n",
    "│├───┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴───┤│\n",
    "││ Tab │ Q │ W │ E │ R │ T │ Y │ U │ I │ O │ P │{[ │}] │ BS  ││\n",
    "│├─────┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴─────┤│\n",
    "││ Ctrl │ A │ S │ D │ F │ G │ H │ J │ K │ L │: ;│\" '│ Enter  ││\n",
    "│├──────┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴────┬───┤│\n",
    "││ Shift  │ Z │ X │ C │ V │ B │ N │ M │< ,│> .│? /│Shift │Fn ││\n",
    "│└─────┬──┴┬──┴──┬┴───┴───┴───┴───┴───┴──┬┴───┴┬──┴┬─────┴───┘│\n",
    "│      │Fn │ Alt │         Space         │ Alt │Win│   HHKB   │\n",
    "│      └───┴─────┴───────────────────────┴─────┴───┘          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "使用T5进行中文问答任务训练，数据集使用百度开源中文问答数据集。\n",
    "\n",
    "Author: pankeyu\n",
    "Date: 2023/01/04\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_scheduler, AdamW\n",
    "from myReader import get_dataLoader,DuReaderQG\n",
    "from bleu_metrics import BLEU\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0  \n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_checkpoint = 'uer/t5-base-chinese-cluecorpussmall'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) \n",
    "model.to(device)\n",
    "# model.load_state_dict(\n",
    "#     torch.load('checkpoints2/epoch_10_valid_rouge_0.0976_model_weights.bin', map_location=device)\n",
    "# )\n",
    "train_data = DuReaderQG('data/DuReaderQG/train.json')\n",
    "valid_data = DuReaderQG('data/DuReaderQG/dev.json')\n",
    "train_dataloader = get_dataLoader(train_data, model, tokenizer, 256, 32, batch_size=32, shuffle=True)\n",
    "valid_dataloader = get_dataLoader(valid_data, model, tokenizer, 256, 32, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b2f27-74c9-48c0-a227-04bde84ecc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c179b201594f44b9e4e13a08c9d875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra2 extra3 extra4 extra5 extra6 extra8 extra9 extra10 extra12 extra7 extra14 extra15 extra13 extra1', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra1 extra12 extra13 extra14 extra15', 'extra0 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra1 extra12 extra13 extra14 extra15', 'extra0 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra1 extra12 extra13 extra14 extra15', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra14', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra8 extra9 extra10 extra7 extra12 extra13 extra14 extra15', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra8 extra9 extra10 extra7 extra12 extra13 extra14 extra15', 'extra0 extra2 extra3 extra9 extra4 extra5 extra6 extra7 extra8 extra14 extra15 extra16 extra1 extra12 extra13', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra12 extra15 extra16 extra13', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra12 extra15 extra16 extra13', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra14', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra6 extra2 extra3 extra4 extra5 extra1 extra8 extra9 extra7 extra14 extra10 extra12 extra15 extra13', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra6 extra2 extra3 extra9 extra4 extra5 extra1 extra8 extra7 extra14 extra15 extra13 extra10 extra12', 'extra0 extra2 extra3 extra4 extra5 extra9 extra6 extra1 extra8 extra7 extra14 extra15 extra13 extra10 extra12', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22', 'extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra22']\n",
      "param_index,param_row, param_column 0 0 0\n",
      "param_index,param_row, param_column 1 0 1\n",
      "param_index,param_row, param_column 2 1 0\n",
      "param_index,param_row, param_column 3 1 1\n",
      "param_index,param_row, param_column 4 2 0\n",
      "{'eval/eval_loss': {'values': [9.2927338077176], 'epochs': [0], 'index': 0}, 'eval/bleu-size-1': {'values': [0.05124879878338915], 'epochs': [0], 'index': 1}, 'eval/bleu-size-2': {'values': [0.01958687699120976], 'epochs': [0], 'index': 2}, 'eval/bleu-size-3': {'values': [0.005554566834660757], 'epochs': [0], 'index': 3}, 'eval/bleu-size-4': {'values': [2.4849849075167803e-79], 'epochs': [0], 'index': 4}}\n",
      "Epoch 1/50\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17627a9b90d342caaba4e38236c6930c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464307d6715941948bd5f342becdd1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['分 中 的 一 个 人 上 大 不 ， 生 小 5 1', '分 中 的 一 个 人 上 大 不 ， 生 小 5 1', '分 上 人 的 一 大 不 个 中 （ 对 三 ， 小', 'extra0 的 分 中 个 上 人 大 不 一 天 ， 到 生', 'extra0 的 分 中 个 上 人 大 不 一 天 ， 到 生', 'extra0 文 个 人 天 中 分 的. 上 ， 大 国 一', '分 的 人 一 大 中 上 个 天 不 （ 生 ， 时', '分 的 人 一 大 中 上 个 天 不 （ 生 ， 时', 'extra0 ， 人 extra8 extra9 extra3 extra4 extra5 extra6 extra7 extra20 extra21 分 的', 'extra0 ， 人 extra8 extra9 extra3 extra4 extra5 extra6 extra7 extra20 extra21 分 的', 'extra0 ， 人 extra8 extra9 extra3 extra4 extra5 extra6 extra7 extra20 extra21 分 的', '分 的 个 一 人 上 不 大 中 extra2 （ ， 天 来', '分 的 人 一 大 extra2 extra3 上 个 中 不 ， 三 5 1', '分 的 extra2 一 大 人 上 中 个 （ 不 ， 生 天', '分 的 extra2 一 大 人 上 中 个 （ 不 ， 生 天', 'extra0 人 一 的 ， 分 个 中 上 出 三 大 天 extra8', 'extra0 人 一 的 ， 分 个 中 上 出 三 大 天 extra8', 'extra0 三 年 本 ， 的 个 分 中 人. 上 天 到 -', 'extra0 市 一 3 公 里 的 个 分 中 天 大 小 -.', 'extra0 市 一 3 公 里 的 个 分 中 天 大 小 -.', '分 上 的 一 三 人 中 个 大 不 （ ， 对 天', '分 一 的 extra2 中 上 人 （ 大 不 个 ， 小 来 对', '分 一 的 extra2 中 上 人 （ 大 不 个 ， 小 来 对', '分 人 的 一 个 上 中 extra2 （ 不 生 大 来 ，', 'extra0 个 人 年 分 日 天 的 大 上 中 不 一 ，', '分 上 的 人 一 大 不 中 个 （ 对 三 ， 生', 'extra0 大 月 日 天 ， 的 分 人 中 一 上 个 年', '分 天 个 日 的 一 上 中 大 三 时 人 周 ，', '分 中 的 大 人 上 个 不 （ 一 ， 小 生 extra2', '分 中 的 大 人 上 个 不 （ 一 ， 小 生 extra2', '分 人 的 中 一 大 上 个 （ 不 ， 生 来 对', '分 人 的 中 一 大 上 个 （ 不 ， 生 来 对']\n",
      "param_index,param_row, param_column 0 0 0\n",
      "param_index,param_row, param_column 1 0 1\n",
      "param_index,param_row, param_column 2 1 0\n",
      "param_index,param_row, param_column 3 1 1\n",
      "param_index,param_row, param_column 4 2 0\n",
      "param_index,param_row, param_column 5 2 1\n",
      "{'eval/eval_loss': {'values': [9.2927338077176, 7.8250349260145615], 'epochs': [0, 1], 'index': 0}, 'eval/bleu-size-1': {'values': [0.05124879878338915, 0.13350171387335377], 'epochs': [0, 1], 'index': 1}, 'eval/bleu-size-2': {'values': [0.01958687699120976, 0.06081150932751776], 'epochs': [0, 1], 'index': 2}, 'eval/bleu-size-3': {'values': [0.005554566834660757, 0.03374036587528652], 'epochs': [0, 1], 'index': 3}, 'eval/bleu-size-4': {'values': [2.4849849075167803e-79, 0.01545372979718752], 'epochs': [0, 1], 'index': 4}, 'train/train_loss': {'values': [9.13428526878357, 8.641415209770203, 8.238678720792135, 7.934825762510299], 'epochs': [100, 200, 300, 400], 'index': 5}}\n",
      "0.01545372979718752\n",
      "Epoch 2/50\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac2ceb6ed3440aba5c80343f3c19fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e3cd88473f401f87114a1b2462e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['主 一 的 分 上 人 个 ， 天 不 大 中 小 来', '主 一 的 分 上 人 个 ， 天 不 大 中 小 来', 'extra0 一 的 分 上 人 ， 个 不 大 中 天 来 小', 'extra0 一 上 分 人 的 ， 个 天 不 大 中 以 行', 'extra0 一 上 分 人 的 ， 个 天 不 大 中 以 行', 'extra0 一 个 中 文 书 院 ，. 的 人 上 分 天 不 个', 'extra0 一 的 分 人 上 ， 个 大 不 三 中 天 行', 'extra0 一 的 分 人 上 ， 个 大 不 三 中 天 行', '主 体 个 ， extra8 extra4 extra5 extra2 人 分 中 天 不 大 一', '主 体 个 ， extra8 extra4 extra5 extra2 人 分 中 天 不 大 一', '主 体 个 ， extra8 extra4 extra5 extra2 人 分 中 天 不 大 一', '主 一 人 分 上 ， 个 的 不 大 天 小 来 中', '主 的 人 分 一 上 个 不 中 大 ， 三 以 天', '主 大 的 分 上 人 ， 个 天 不 一 小 中 来', '主 大 的 分 上 人 ， 个 天 不 一 小 中 来', 'extra0 人 ， 个 的 分 上 了. 一 三 中 小 不', 'extra0 人 ， 个 的 分 上 了. 一 三 中 小 不', 'extra0 年 个 三 月 天 ， 的 - 2 年 。 上 人 一 百 三 年', 'extra0. 2 公 里 ， 的 个 人 分 1. 3 - 2. 5 公 公', 'extra0. 2 公 里 ， 的 个 人 分 1. 3 - 2. 5 公 公', '主 一 ， 分 上 个 人 大 的 不 小 （ 年 中', '主 的 人 分 上 个 一 ， 不 大 小 三 中 天', '主 的 人 分 上 个 一 ， 不 大 小 三 中 天', '主 的 分 上 ， 个 不 人 一 大 中 天 来 （', 'extra0 天 年 一 月 15 日 ， 人 个 的 - 5 月. 5 年 （', '主 上 ， 人 一 分 的 大 不 中 个 天 来 三', 'extra0 日 年 11 月 8 日 到 10 月 10 日 - 14 月 日 月 5 月 6 日 的 天 一', 'extra0 日 天 个 月 23 日 周 一 年 ， - 3 月 3 的 人', '主 一 ， 人 分 上 大 的 小 不 中 天 来 三', '主 一 ， 人 分 上 大 的 小 不 中 天 来 三', '主 一 的 ， 分 人 上 个 三 大 不 中 天 生', '主 一 的 ， 分 人 上 个 三 大 不 中 天 生']\n",
      "param_index,param_row, param_column 0 0 0\n",
      "param_index,param_row, param_column 1 0 1\n",
      "param_index,param_row, param_column 2 1 0\n",
      "param_index,param_row, param_column 3 1 1\n",
      "param_index,param_row, param_column 4 2 0\n",
      "param_index,param_row, param_column 5 2 1\n",
      "{'eval/eval_loss': {'values': [9.2927338077176, 7.8250349260145615, 7.072479683865783], 'epochs': [0, 1, 2], 'index': 0}, 'eval/bleu-size-1': {'values': [0.05124879878338915, 0.13350171387335377, 0.14995834490419327], 'epochs': [0, 1, 2], 'index': 1}, 'eval/bleu-size-2': {'values': [0.01958687699120976, 0.06081150932751776, 0.08435027401968866], 'epochs': [0, 1, 2], 'index': 2}, 'eval/bleu-size-3': {'values': [0.005554566834660757, 0.03374036587528652, 0.057467640978051786], 'epochs': [0, 1, 2], 'index': 3}, 'eval/bleu-size-4': {'values': [2.4849849075167803e-79, 0.01545372979718752, 0.040191494410377525], 'epochs': [0, 1, 2], 'index': 4}, 'train/train_loss': {'values': [9.13428526878357, 8.641415209770203, 8.238678720792135, 7.934825762510299, 7.688117396354675, 7.482095204194387, 7.311326945849827, 7.164523186683655, 7.034662363264296], 'epochs': [100, 200, 300, 400, 500, 600, 700, 800, 900], 'index': 5}}\n",
      "0.040191494410377525\n",
      "Epoch 3/50\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84acd4431664eb5afce4ab3ed32e01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from iTrainingLogger import iSummaryWriter\n",
    "writer = iSummaryWriter(log_path='logs/DuReaderQG2', log_name='DuReaderQG2')\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        loss_avg = sum(train_losses) / len(train_losses)\n",
    "        progress_bar.set_description(f'loss: {loss_avg:>7f}')\n",
    "        progress_bar.update(1)\n",
    "        if len(train_losses) % 100 == 0:\n",
    "            writer.add_scalar('train/train_loss', loss_avg, finish_batch_num + batch)\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, epoch):\n",
    "    max_target_length = 32\n",
    "    bleu_evaluators = [BLEU(n_size=i+1) for i in range(4)]\n",
    "    preds, labels = [], []\n",
    "    model.eval()\n",
    "    a= True\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs = model(**batch_data)\n",
    "            loss = outputs.loss\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "            ).cpu().numpy()\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        if a:\n",
    "            print(decoded_preds)\n",
    "            a=False\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "        for bleu_evaluator in bleu_evaluators:\n",
    "            for pred,label in zip(decoded_preds,decoded_labels):\n",
    "                bleu_evaluator.add_instance(prediction=pred.strip(), references=[label.strip()])\n",
    "    loss_avg = sum(test_losses) / len(test_losses)\n",
    "    bleu1, bleu2, bleu3, bleu4 = [bleu.compute() for bleu in bleu_evaluators]\n",
    "    writer.add_scalar('eval/eval_loss', loss_avg, epoch)\n",
    "    writer.add_scalar('eval/bleu-size-1', bleu1, epoch)\n",
    "    writer.add_scalar('eval/bleu-size-2', bleu2, epoch)\n",
    "    writer.add_scalar('eval/bleu-size-3', bleu3, epoch)\n",
    "    writer.add_scalar('eval/bleu-size-4', bleu4, epoch)\n",
    "    writer.record()\n",
    "    return bleu4    \n",
    "learning_rate = 5e-5\n",
    "epoch_num = 50\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "best_bleu4 = 0.\n",
    "test_loop(valid_dataloader, model, 0)\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1)\n",
    "    bleu4 = test_loop(valid_dataloader, model, t+1)\n",
    "    print(bleu4)\n",
    "    if bleu4 > best_bleu4:\n",
    "        best_bleu4 = bleu4\n",
    "        cur_save_dir = \"model_best2\"\n",
    "        if not os.path.exists(cur_save_dir):\n",
    "            os.makedirs(cur_save_dir)\n",
    "        model.save_pretrained(os.path.join(cur_save_dir))\n",
    "        tokenizer.save_pretrained(os.path.join(cur_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3a9aa-2f18-4f19-aa1b-698e326eec07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
